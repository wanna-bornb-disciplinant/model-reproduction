{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16922613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributed import dist\n",
    "\n",
    "def zeropower_via_newtonschulz5(G, steps:int):\n",
    "    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng\n",
    "    a, b, c = (3.4445, -4.7750,  2.0315)\n",
    "    X = G.bfloat16()\n",
    "    if G.size(-2) > G.size(-1): # when G.dim > 2, ensure the last 2 dims is the rows and columns of the matrix \n",
    "        X = X.mT\n",
    "\n",
    "    # Ensure spectral norm is at most 1\n",
    "    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)\n",
    "    # Perform the NS iterations\n",
    "    for _ in range(steps):\n",
    "        A = X @ X.mT\n",
    "        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng\n",
    "        X = a * X + B @ X\n",
    "    \n",
    "    if G.size(-2) > G.size(-1):\n",
    "        X = X.mT\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ab50d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def muon_update(grad, momentum, beta=0.95, ns_steps=5, nesterov=True):\n",
    "    momentum.lerp_(grad, 1 - beta)\n",
    "    update = grad.lerp_(momentum, beta) if nesterov else momentum\n",
    "    if update.ndim == 4: # for the case of conv filters\n",
    "        update = update.view(len(update), -1)\n",
    "    update = zeropower_via_newtonschulz5(update, steps=ns_steps)\n",
    "    update *= max(1, grad.size(-2) / grad.size(-1))**0.5\n",
    "    return update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9333eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Muon(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Muon - MomentUm Orthogonalized by Newton-schulz\n",
    "\n",
    "    https://kellerjordan.github.io/posts/muon/\n",
    "\n",
    "    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n",
    "    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n",
    "    matrix. For efficient orthogonalization we use a Newton-Schulz iteration, which has the\n",
    "    advantage that it can be stably run in bfloat16 on the GPU.\n",
    "\n",
    "    Muon should only be used for hidden weight layers. The input embedding, final output layer,\n",
    "    and any internal gains or biases should be optimized using a standard method such as AdamW.\n",
    "    Hidden convolutional weights can be trained using Muon by viewing them as 2D and then\n",
    "    collapsing their last 3 dimensions.\n",
    "\n",
    "    Arguments:\n",
    "        lr: The learning rate, in units of spectral norm per update.\n",
    "        weight_decay: The AdamW-style weight decay.\n",
    "        momentum: The momentum. A value of 0.95 here is usually fine.\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=0.02, weight_decay=0, momentum=0.95):\n",
    "        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)\n",
    "        assert isinstance(params, list) and len(params) >= 1 and isinstance(params[0], torch.nn.Parameter)\n",
    "        params = sorted(params, key=lambda x: x.size(), reverse=True)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            params = group[\"params\"]\n",
    "            params_pad = params + [torch.empty_like(params[-1])] * (dist.get_world_size() - len(params) % dist.get_world_size())\n",
    "            for base_i in range(len(params))[::dist.get_world_size()]:\n",
    "                if base_i + dist.get_rank() < len(params):\n",
    "                    p = params[base_i + dist.get_rank()]\n",
    "                    state = self.state[p]\n",
    "                    if len(state) == 0:\n",
    "                        state[\"momentum_buffer\"] = torch.zeros_like(p)\n",
    "                    update = muon_update(p.grad, state[\"momentum_buffer\"], beta=group[\"momentum\"])\n",
    "                    p.mul_(1 - group[\"lr\"] * group[\"weight_decay\"])\n",
    "                    p.add_(update, alpha=-group[\"lr\"])\n",
    "                dist.all_gather(params_pad[base_i:base_i + dist.get_world_size()], params_pad[base_i + dist.get_rank()])\n",
    "\n",
    "\n",
    "class SingleDeviceMuon(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Muon variant for usage in non-distributed settings.\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=0.02, weight_decay=0, momentum=0.95):\n",
    "        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                state = self.state[p]\n",
    "                if len(state) == 0:\n",
    "                    state[\"momentum_buffer\"] = torch.zeros_like(p)\n",
    "                update = muon_update(p.grad, state[\"momentum_buffer\"], beta=group[\"momentum\"])\n",
    "                p.mul_(1 - group[\"lr\"] * group[\"weight_decay\"])\n",
    "                p.add_(update, alpha=-group[\"lr\"])\n",
    "\n",
    "\n",
    "def adam_update(grad, buf1, buf2, step, betas, eps):\n",
    "    buf1.lerp_(grad, 1 - betas[0])\n",
    "    buf2.lerp_(grad.square(), 1 - betas[1])\n",
    "    buf1c = buf1 / (1 - betas[0]**step)\n",
    "    buf2c = buf2 / (1 - betas[1]**step)\n",
    "    return buf1c / (buf2c.sqrt() + eps)\n",
    "\n",
    "\n",
    "class MuonWithAuxAdam(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Distributed Muon variant that can be used for all parameters in the network, since it runs an\n",
    "    internal AdamW for the parameters that are not compatible with Muon. The user must manually\n",
    "    specify which parameters shall be optimized with Muon and which with Adam by passing in a\n",
    "    list of param_groups with the `use_muon` flag set.\n",
    "\n",
    "    The point of this class is to allow the user to have a single optimizer in their code, rather\n",
    "    than having both a Muon and an Adam which each need to be stepped.\n",
    "\n",
    "    You can see an example usage below:\n",
    "\n",
    "    https://github.com/KellerJordan/modded-nanogpt/blob/master/records/052525_MuonWithAuxAdamExample/b01550f9-03d8-4a9c-86fe-4ab434f1c5e0.txt#L470\n",
    "    ```\n",
    "    hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and \"embed\" not in n]\n",
    "    embed_params = [p for n, p in model.named_parameters() if \"embed\" in n]\n",
    "    scalar_params = [p for p in model.parameters() if p.ndim < 2]\n",
    "    head_params = [model.lm_head.weight]\n",
    "\n",
    "    from muon import MuonWithAuxAdam\n",
    "    adam_groups = [dict(params=head_params, lr=0.22), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]\n",
    "    adam_groups = [dict(**g, betas=(0.8, 0.95), eps=1e-10, use_muon=False) for g in adam_groups]\n",
    "    muon_group = dict(params=hidden_matrix_params, lr=0.05, momentum=0.95, use_muon=True)\n",
    "    param_groups = [*adam_groups, muon_group]\n",
    "    optimizer = MuonWithAuxAdam(param_groups)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    def __init__(self, param_groups):\n",
    "        for group in param_groups:\n",
    "            assert \"use_muon\" in group\n",
    "            if group[\"use_muon\"]:\n",
    "                group[\"params\"] = sorted(group[\"params\"], key=lambda x: x.size(), reverse=True)\n",
    "                # defaults\n",
    "                group[\"lr\"] = group.get(\"lr\", 0.02)\n",
    "                group[\"momentum\"] = group.get(\"momentum\", 0.95)\n",
    "                group[\"weight_decay\"] = group.get(\"weight_decay\", 0)\n",
    "                assert set(group.keys()) == set([\"params\", \"lr\", \"momentum\", \"weight_decay\", \"use_muon\"])\n",
    "            else:\n",
    "                # defaults\n",
    "                group[\"lr\"] = group.get(\"lr\", 3e-4)\n",
    "                group[\"betas\"] = group.get(\"betas\", (0.9, 0.95))\n",
    "                group[\"eps\"] = group.get(\"eps\", 1e-10)\n",
    "                group[\"weight_decay\"] = group.get(\"weight_decay\", 0)\n",
    "                assert set(group.keys()) == set([\"params\", \"lr\", \"betas\", \"eps\", \"weight_decay\", \"use_muon\"])\n",
    "        super().__init__(param_groups, dict())\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            if group[\"use_muon\"]:\n",
    "                params = group[\"params\"]\n",
    "                params_pad = params + [torch.empty_like(params[-1])] * (dist.get_world_size() - len(params) % dist.get_world_size())\n",
    "                for base_i in range(len(params))[::dist.get_world_size()]:\n",
    "                    if base_i + dist.get_rank() < len(params):\n",
    "                        p = params[base_i + dist.get_rank()]\n",
    "                        state = self.state[p]\n",
    "                        if len(state) == 0:\n",
    "                            state[\"momentum_buffer\"] = torch.zeros_like(p)\n",
    "                        update = muon_update(p.grad, state[\"momentum_buffer\"], beta=group[\"momentum\"])\n",
    "                        p.mul_(1 - group[\"lr\"] * group[\"weight_decay\"])\n",
    "                        p.add_(update, alpha=-group[\"lr\"])\n",
    "                    dist.all_gather(params_pad[base_i:base_i + dist.get_world_size()], params_pad[base_i + dist.get_rank()])\n",
    "            else:\n",
    "                for p in group[\"params\"]:\n",
    "                    state = self.state[p]\n",
    "                    if len(state) == 0:\n",
    "                        state[\"exp_avg\"] = torch.zeros_like(p)\n",
    "                        state[\"exp_avg_sq\"] = torch.zeros_like(p)\n",
    "                        state[\"step\"] = 0\n",
    "                    state[\"step\"] += 1\n",
    "                    update = adam_update(p.grad, state[\"exp_avg\"], state[\"exp_avg_sq\"],\n",
    "                                         state[\"step\"], group[\"betas\"], group[\"eps\"])\n",
    "                    p.mul_(1 - group[\"lr\"] * group[\"weight_decay\"])\n",
    "                    p.add_(update, alpha=-group[\"lr\"])\n",
    "\n",
    "class SingleDeviceMuonWithAuxAdam(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Non-distributed variant of MuonWithAuxAdam.\n",
    "    \"\"\"\n",
    "    def __init__(self, param_groups):\n",
    "        for group in param_groups:\n",
    "            assert \"use_muon\" in group\n",
    "            if group[\"use_muon\"]:\n",
    "                # defaults\n",
    "                group[\"lr\"] = group.get(\"lr\", 0.02)\n",
    "                group[\"momentum\"] = group.get(\"momentum\", 0.95)\n",
    "                group[\"weight_decay\"] = group.get(\"weight_decay\", 0)\n",
    "                assert set(group.keys()) == set([\"params\", \"lr\", \"momentum\", \"weight_decay\", \"use_muon\"])\n",
    "            else:\n",
    "                # defaults\n",
    "                group[\"lr\"] = group.get(\"lr\", 3e-4)\n",
    "                group[\"betas\"] = group.get(\"betas\", (0.9, 0.95))\n",
    "                group[\"eps\"] = group.get(\"eps\", 1e-10)\n",
    "                group[\"weight_decay\"] = group.get(\"weight_decay\", 0)\n",
    "                assert set(group.keys()) == set([\"params\", \"lr\", \"betas\", \"eps\", \"weight_decay\", \"use_muon\"])\n",
    "        super().__init__(param_groups, dict())\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            if group[\"use_muon\"]:\n",
    "                for p in group[\"params\"]:\n",
    "                    state = self.state[p]\n",
    "                    if len(state) == 0:\n",
    "                        state[\"momentum_buffer\"] = torch.zeros_like(p)\n",
    "                    update = muon_update(p.grad, state[\"momentum_buffer\"], beta=group[\"momentum\"])\n",
    "                    p.mul_(1 - group[\"lr\"] * group[\"weight_decay\"])\n",
    "                    p.add_(update, alpha=-group[\"lr\"])\n",
    "            else:\n",
    "                for p in group[\"params\"]:\n",
    "                    state = self.state[p]\n",
    "                    if len(state) == 0:\n",
    "                        state[\"exp_avg\"] = torch.zeros_like(p)\n",
    "                        state[\"exp_avg_sq\"] = torch.zeros_like(p)\n",
    "                        state[\"step\"] = 0\n",
    "                    state[\"step\"] += 1\n",
    "                    update = adam_update(p.grad, state[\"exp_avg\"], state[\"exp_avg_sq\"],\n",
    "                                         state[\"step\"], group[\"betas\"], group[\"eps\"])\n",
    "                    p.mul_(1 - group[\"lr\"] * group[\"weight_decay\"])\n",
    "                    p.add_(update, alpha=-group[\"lr\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
