{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7484bcfc",
   "metadata": {},
   "source": [
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.90, 0.95), weight_decay=0.01)\n",
    "\n",
    "# To replace the above, do the following:\n",
    "from muon import MuonWithAuxAdam\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Sequential(nn.Linear(100,2)) # for example\n",
    " \n",
    "\n",
    "hidden_weights = [p for p in model.body.parameters() if p.ndim >= 2]\n",
    "hidden_gains_biases = [p for p in model.body.parameters() if p.ndim < 2]\n",
    "nonhidden_params = [*model.head.parameters(), *model.embed.parameters()]\n",
    "param_groups = [\n",
    "    dict(params=hidden_weights, use_muon=True,\n",
    "         lr=0.02, weight_decay=0.01),\n",
    "    dict(params=hidden_gains_biases+nonhidden_params, use_muon=False,\n",
    "         lr=3e-4, betas=(0.9, 0.95), weight_decay=0.01),\n",
    "]\n",
    "optimizer = MuonWithAuxAdam(param_groups)"
   ],
   "outputs":[

   ]
  },
  {
   "cell_type": "markdown",
   "id": "6378db95",
   "metadata": {},
   "source": [
    "Muon is an optimizer for the hidden weights of a neural network. Other parameters, such as embeddings, classifier heads, and hidden gains/biases should be optimized using standard AdamW. \n",
    "\n",
    "You'll have to replace ```model.body```, ```model.head```, and ```model.embed``` with whatever is appropriate for your model. E.g., for a ConvNet, you should use Muon to optimize all the convolutional filters except the first one, and AdamW to optimize everything else."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
